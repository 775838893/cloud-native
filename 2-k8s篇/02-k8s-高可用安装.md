# k8s集群-普通安装

## 准备工作

注意事项:

> 1.安装k8s前请先安装好docker，具体安装方法参考文档<<docker生产安装.md>>。
> 2.文档中IP地址为示意地址，安装时请替换为实际生产地址。
> 3.本文档不要一次性执行一个命令框（灰色框）内的全部命令，应按照步骤说明分步执行。s

规划机器。操作系统：CentOS Linux release 7.9.2009 (Core)

```
192.168.1.11 master
192.168.1.12 node1
192.168.1.13 node2
```

提前准备docker镜像

```powershell
#拉取镜像
docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.19.7
docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.7
docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.7
docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.7 
docker pull registry.aliyuncs.com/google_containers/etcd:3.4.13-0 
docker pull registry.aliyuncs.com/google_containers/coredns:1.7.0
docker pull registry.aliyuncs.com/google_containers/pause:3.2
docker pull calico/node:v3.17.2
docker pull calico/pod2daemon-flexvol:v3.17.2
docker pull calico/cni:v3.17.2           
docker pull calico/kube-controllers:v3.17.2
docker pull kubernetesui/dashboard:v2.0.0-rc7
docker pull kubernetesui/metrics-scraper:v1.0.4

#保存镜像
docker save -o  ./tars/1.tar registry.aliyuncs.com/google_containers/kube-proxy:v1.19.7
docker save -o  ./tars/2.tar registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.7
docker save -o  ./tars/3.tar registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.7
docker save -o  ./tars/4.tar registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.7 
docker save -o  ./tars/5.tar registry.aliyuncs.com/google_containers/etcd:3.4.13-0 
docker save -o  ./tars/6.tar registry.aliyuncs.com/google_containers/coredns:1.7.0
docker save -o  ./tars/7.tar registry.aliyuncs.com/google_containers/pause:3.2
docker save -o  ./tars/9.tar calico/node:v3.17.2
docker save -o  ./tars/10.tar calico/pod2daemon-flexvol:v3.17.2
docker save -o  ./tars/11.tar calico/cni:v3.17.2           
docker save -o  ./tars/12.tar calico/kube-controllers:v3.17.2
docker save -o  ./tars/13.tar kubernetesui/dashboard:v2.0.0-rc7
docker save -o  ./tars/14.tar kubernetesui/metrics-scraper:v1.0.4

```

拷贝文件

```powershell
#拷贝文件都3台机器
scp -r tars root@192.168.1.11:/root/
scp -r tars root@192.168.1.12:/root/
scp -r tars root@192.168.1.13:/root/
```

设置hostname

```powershell
#机器192.168.2.11执行
hostnamectl --static set-hostname  master

#机器192.168.2.12执行
hostnamectl --static set-hostname  node1

#机器192.168.2.13执行
hostnamectl --static set-hostname  node2

```

```powershell
#所有机器上执行，hosts文件追加本地解析记录
cat <<EOF >>  /etc/hosts
192.168.1.11    master
192.168.1.12    node1
192.168.1.13    node2
EOF

cat /etc/hosts
```

## 安装步骤

*以下命令在所有机器执行*

```powershell
#### 关闭防火墙
systemctl stop firewalld.service
systemctl status firewalld.service
systemctl disable firewalld

#### 关闭Swap
swapoff -a
sed -ri 's/.*swap.*/#&/' /etc/fstab
echo "vm.swappiness = 0" >> /etc/sysctl.conf 
sysctl -p

#### 关闭selinux
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config

#### 设置启动参数
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system
```

## 安装docker

以下命令在所有机器执行

```powershell
#离线安装
yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# 下载docker-ce-18.06.3.ce-3.e17离线安装包
yum install --downloadonly --downloaddir=/root/docker docker-ce-18.06.3.ce-3.e17
# 离线安装命令
rpm -ivh /root/docker/*.rpm
```



```powershell
#在线安装
yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum list docker-ce --showduplicates | sort -r
yum install -y docker-ce-18.06.3.ce-3.el7
systemctl start docker
systemctl enable docker
```

设置docker的cgroupdriver为systemd

```powershell
# Set up the Docker daemon
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

#重启
systemctl restart docker
```

导入提前准备好的镜像

```powershell
#导入镜像脚本
docker load < ./tars/1.tar
docker load < ./tars/2.tar
docker load < ./tars/3.tar
docker load < ./tars/4.tar
docker load < ./tars/5.tar
docker load < ./tars/6.tar
docker load < ./tars/7.tar
docker load < ./tars/9.tar
docker load < ./tars/10.tar
docker load < ./tars/11.tar
docker load < ./tars/12.tar
docker load < ./tars/13.tar
docker load < ./tars/14.tar
```

## 安装kubeadm,kubelet,kubectl

以下命令在所有机器执行

```powershell
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet-1.19.7 kubeadm-1.19.7 kubectl-1.19.7
systemctl enable kubelet && systemctl start kubelet
```

```powershell
#离线安装
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

#下载kubelet-1.19.7 kubeadm-1.19.7 kubectl-1.19.7离线安装包
yum install --downloadonly --downloaddir=/root/k8s kubelet-1.19.7 kubeadm-1.19.7 kubectl-1.19.7
#离线安装命令
rpm -ivh /root/k8s/*.rpm
```



## master节点安装

*以下命令在master 节点安装*

### 初始化Master

```powershell
# 初始化master ，添加--image-repository 参数，默认镜像会下载失败
kubeadm init \
  --apiserver-advertise-address=$masterIp \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.19.7 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16 \
  --ignore-preflight-errors=all
```

```powershell
#如果init成功执行如下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 安装 calico 网络

```powershell
# 安装网络
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# 本地运行
kubectl apply -f tars/calico.yaml

#查看集群
kubectl get pods -n kube-system
```

### 安装dashboard

```powershell
#vim recommended.yaml

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30000
  selector:
    k8s-app: kubernetes-dashboard
    
#安装dashboard
kubectl create -f tars/recommended.yaml
```

查看pod

```powershell
kubectl get pods -n kubernetes-dashboard
```

使用token进行登录，使用下面命令获取token

```powershell
#命令获取token
kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
```

访问

htttp://ip:30000



## node节点安装

*以下命令在Node节点安装*

```powershell
#master 节点所有命令执行完毕后，node节点加入集群
kubeadm join 192.168.1.11:6443 --token m0rpym.522tija0299geb8h \ 
--discovery-token-ca-cert-hash sha256:cef7351d9fefc67868f22aa3122165dd01f63e 95870d2fb22197ee66c61b18d6
```

查看是否加入

1.在浏览器访问 htttp://ip:30000

2.在master节点上查看

```
kubectl get nodes
```



## master卸载

```powershell
#重置节点
kubeadm reset
# 这个命令在master, node都执行

#删除目录
rm -rf $HOME/.kube
# 这个命令在master, node都执行
```



# k8s集群-master节点脚本安装

```
参考: k8s-离线安装脚本.md
```



# k8s高可用集群

## 准备工作

规划机器。操作系统：CentOS Linux release 7.9.2009 (Core)

```
192.168.68.164 master01
192.168.68.165 master02
192.168.68.167 master03
```

拷贝文件

```powershell
#拷贝文件都3台机器
scp -r install-k8s/ root@192.168.68.164:/root/
scp -r install-k8s/ root@192.168.68.165:/root/
scp -r install-k8s/ root@192.168.68.167:/root/

#拷贝的文件如下：
install-k8s/tars/*
install-k8s/initMaster.sh
install-k8s/step1-initServer.sh
```

设置hostname

```powershell
#机器192.168.68.164执行
hostnamectl --static set-hostname  master01

#机器192.168.68.165执行
hostnamectl --static set-hostname  master02

#机器192.168.68.167执行
hostnamectl --static set-hostname  master03
```



## 虚拟机设置

#执行初始化脚本

下面3个机器都执行

sh step1-initServer.sh

```shell
#!/bin/bash

#### 关闭防火墙
systemctl stop firewalld.service
systemctl status firewalld.service
systemctl disable firewalld

#### 关闭Swap
swapoff -a
sed -ri 's/.*swap.*/#&/' /etc/fstab
echo "vm.swappiness = 0" >> /etc/sysctl.conf 
sysctl -p

#### 关闭selinux
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config

#### 设置启动参数
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

#### 安装docker

#离线安装参考《docker生产安装》
tar -zxvf tars/docker-18.06.3-ce.tgz 
cp docker/* /usr/bin/

cat <<EOF >  /etc/systemd/system/docker.service
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
# the default is not to use systemd for cgroups because the delegate issues still
# exists and systemd currently does not support the cgroup feature set required
# for containers run by docker
ExecStart=/usr/bin/dockerd --selinux-enabled=false
ExecReload=/bin/kill -s HUP $MAINPID
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
#TasksMax=infinity
TimeoutStartSec=0
# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes
# kill only the docker process, not all processes in the cgroup
KillMode=process
# restart the docker process if it exits prematurely
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
EOF

#加执行权限
chmod +x /etc/systemd/system/docker.service
systemctl daemon-reload
#启动
systemctl start docker

#设置开机启动
systemctl enable docker.service

#查看docker服务状态
systemctl status docker

#设置docker
cat <<EOF > /etc/docker/daemon.json
{
  "registry-mirrors": ["http://hub-mirror.c.163.com"],
  "insecure-registries": ["10.7.92.101:5000"]
}
EOF

#重启
systemctl restart docker

#导入镜像脚本
docker load < ./tars/1.tar
docker load < ./tars/2.tar
docker load < ./tars/3.tar
docker load < ./tars/4.tar
docker load < ./tars/5.tar
docker load < ./tars/6.tar
docker load < ./tars/7.tar
docker load < ./tars/9.tar
docker load < ./tars/10.tar
docker load < ./tars/11.tar
docker load < ./tars/12.tar
docker load < ./tars/13.tar
docker load < ./tars/14.tar


#离线安装kubelet-1.19.7,kubeadm-1.19.7,kubectl-1.19.7
rpm -ivh ./tars/k8s/*.rpm

systemctl enable kubelet && systemctl start kubelet

```



## 安装haproxy和keepalive

### 创建 HAProxy 启动脚本

> 该步骤在 `master01 master02 master03` 执行

```powershell
mkdir -p /usr/local/kubernetes/lb
vi /usr/local/kubernetes/lb/start-haproxy.sh

# 输入内容如下
#!/bin/bash
# 修改为你自己的 Master 地址
MasterIP1=192.168.68.164
MasterIP2=192.168.68.165
MasterIP3=192.168.68.167
# 这是 kube-apiserver 默认端口，不用修改
MasterPort=6443

# 容器将 HAProxy 的 6444 端口暴露出去
docker run -d --restart=always --name HAProxy-K8S -p 6444:6444 \
        -e MasterIP1=$MasterIP1 \
        -e MasterIP2=$MasterIP2 \
        -e MasterIP3=$MasterIP3 \
        -e MasterPort=$MasterPort \
        wise2c/haproxy-k8s

# 设置权限
chmod +x start-haproxy.sh
```

### 创建 Keepalived 启动脚本

> 该步骤在 `master01 master02 master03` 执行

```powershell
mkdir -p /usr/local/kubernetes/lb
vi /usr/local/kubernetes/lb/start-keepalived.sh

# 输入内容如下
#!/bin/bash
# 修改为你自己的虚拟 IP 地址
VIRTUAL_IP=192.168.68.200
# 虚拟网卡设备名
INTERFACE=ens33
# 虚拟网卡的子网掩码
NETMASK_BIT=24
# HAProxy 暴露端口，内部指向 kube-apiserver 的 6443 端口
CHECK_PORT=6444
# 路由标识符
RID=10
# 虚拟路由标识符
VRID=160
# IPV4 多播地址，默认 224.0.0.18
MCAST_GROUP=224.0.0.18

docker run -itd --restart=always --name=Keepalived-K8S \
        --net=host --cap-add=NET_ADMIN \
        -e VIRTUAL_IP=$VIRTUAL_IP \
        -e INTERFACE=$INTERFACE \
        -e CHECK_PORT=$CHECK_PORT \
        -e RID=$RID \
        -e VRID=$VRID \
        -e NETMASK_BIT=$NETMASK_BIT \
        -e MCAST_GROUP=$MCAST_GROUP \
        wise2c/keepalived-k8s

# 设置权限
chmod +x start-keepalived.sh
```

```powershell
docker load < haproxy.tar

docker load < keepalived.tar
1）每个节点执行,先执行这个
sh start-haproxy.sh
2) 每个节点执行，上面执行完再执行这个
sh start-keepalived.sh

ip a | grep ens
192.168.68.200 这个ip在才是OK的
```



## 初始化 Master

> 该步骤在 `master01` 执行

* 创建工作目录并导出配置文件

```powershell
# 导出配置文件到工作目录
kubeadm config print init-defaults --kubeconfig ClusterConfiguration > kubeadm.yml
```

`vi kubeadm.yml`

```yaml
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  # 修改为主节点 IP
  advertiseAddress: 192.168.68.164
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: kubernetes-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
# 配置 Keepalived 地址和 HAProxy 端口
controlPlaneEndpoint: "192.168.68.200:6444"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
# 国内不能访问 Google，修改为阿里云
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
# 修改版本号
kubernetesVersion: v1.19.7
networking:
  dnsDomain: cluster.local
  # 配置成 Calico 的默认网段
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
scheduler: {}

```



```powershell
# kubeadm 初始化
kubeadm init --config=kubeadm.yml | tee kubeadm-init.log

# 配置 kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 安装网络
kubectl apply -f tars/calico.yaml


#查看集群
kubectl get pods -n kube-system
```



## 节点加入

> 该步骤在 ` master02 master03` 执行

```powershell
#master02 master03创建文件夹
mkdir -p /etc/kubernetes/pki/etcd/

#拷贝master01上的证书到机器master02
scp /etc/kubernetes/pki/ca.* root@192.168.68.165:/etc/kubernetes/pki
scp /etc/kubernetes/pki/sa.* root@192.168.68.165:/etc/kubernetes/pki
scp /etc/kubernetes/pki/front-proxy-ca.* root@192.168.68.165:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/etcd/ca.* root@192.168.68.165:/etc/kubernetes/pki/etcd/
scp /etc/kubernetes/admin.conf root@192.168.68.165:/etc/kubernetes/


#拷贝master01上的证书到机器master03
scp /etc/kubernetes/pki/ca.* root@192.168.68.167:/etc/kubernetes/pki
scp /etc/kubernetes/pki/sa.* root@192.168.68.167:/etc/kubernetes/pki
scp /etc/kubernetes/pki/front-proxy-ca.* root@192.168.68.167:/etc/kubernetes/pki/
scp /etc/kubernetes/pki/etcd/ca.* root@192.168.68.167:/etc/kubernetes/pki/etcd/
scp /etc/kubernetes/admin.conf root@192.168.68.167:/etc/kubernetes/

#在文件kubeadm-init.log找到如下命令，将master02 master03加入
#其他master加入执行如下命令
kubeadm join 192.168.68.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:9ee9c17195287307cdf8bf8658b597d71fc788d69fe34df885fbd41986db6a5a \
    --control-plane 
#加入成功后master02 master03执行如下命令
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#node加入执行
kubeadm join 192.168.68.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:9ee9c17195287307cdf8bf8658b597d71fc788d69fe34df885fbd41986db6a5a

#master01 验证
kubectl get nodes
```



## 安装dashboard

> 该步骤在 master01  执行

```powershell
#安装dashboard
kubectl create -f tars/recommended.yaml

kubectl get pods -n kubernetes-dashboard

#命令获取token
kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
```

